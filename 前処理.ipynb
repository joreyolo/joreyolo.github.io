{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zeroc-ice\n",
      "  Downloading zeroc_ice-3.7.4-cp38-cp38-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: zeroc-ice\n",
      "Successfully installed zeroc-ice-3.7.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\yu_wang\\anaconda3\\envs\\python38_new\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install zeroc-ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/fronteo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ①よくある文章のノーマライゼーション処理\n",
    "    ・参考文献：\n",
    "        ①https://mp.weixin.qq.com/s/Gq_0Ks3XTUzZSFYjRrcfMQ\n",
    "        ②https://www.jianshu.com/p/0e1d51a7549d\n",
    "    ・大文字・小文字の統一\n",
    "    ・数字を単語にするか、削除\n",
    "    ・文章区切り等記号の処理\n",
    "    ・空白の削除\n",
    "    ・省略語の修正\n",
    "    ・特殊表現の処理（削除等）\n",
    "    ・text canonicalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ①大文字・小文字の統一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = 'The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.'\n",
    "input_str = input_str.lower()\n",
    "print(input_str)\n",
    "input_str = input_str.upper()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ②数字を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "input_str = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n",
    "result = re.sub(r'\\d+','',input_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ★★★★➂文章区切り等記号の処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "input_str = 'This &is [an] example? {of} string. with.? punctuation!!!!'\n",
    "trans=str.maketrans({key: None for key in string.punctuation})\n",
    "result=input_str.translate(trans)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ④空白を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = ' \\t a string example\\t '\n",
    "input_str = input_str.strip()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ④stopwords\n",
    "    nltk,scikit-learn,spaCy等stopwordsを提供してくれる\n",
    "    from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#英文(nltk)  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "input_str = 'NLTK is a leading platform for building Python programs to work with human language data.'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ➄単語語根の抽出(Stemming)\n",
    "単語の語根（books→book　looked→look）\n",
    "2種類のやり方：\n",
    "Porter stemming（単語内のよくある形態または語尾を削除）\n",
    "Lancaster stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTKを使って語根を抽出する\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "input_str = 'There are several types of stemming algorithms.'\n",
    "input_str = word_tokenize(input_str) #分かち書き\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ➅単語のステムリダクション（詞幹還原）\n",
    "    単語のステムリダクションとは、単語ごとの形態を統一するためである。語根抽出と異なり、単語のステムリダクションはモジュールを使って単語の形態を変更する\n",
    "    \n",
    "    当前常用的词形还原工具库包括： NLTK（WordNet Lemmatizer），spaCy，TextBlob，Pattern，gensim，Stanford CoreNLP，基于内存的浅层解析器（MBSP），Apache OpenNLP，Apache Lucene，文本工程通用架构（GATE），Illinois Lemmatizer 和 DKPro Core。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "input_str = 'been had done languages cities mice better'\n",
    "input_str = word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print('posあり:'+str(lemmatizer.lemmatize(word,pos = 'a')))\n",
    "     \n",
    "    print('posなし:'+str(lemmatizer.lemmatize(word)))\n",
    "    \n",
    "    print('======================================')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ⑦詞性標註 Part-of-speech tagging（POC）\n",
    "    当前有许多包含 POS 标记器的工具，包括 NLTK，spaCy，TextBlob，Pattern，Stanford CoreNLP，基于内存的浅层分析器（MBSP），Apache OpenNLP，Apache Lucene，文本工程通用架构（GATE），FreeLing，Illinois Part of Speech Tagger 和 DKPro Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "#textblobを使ってPOCを行う\n",
    "input_str = 'Parts of speech examples: an article, to write, interesting, easily, and, of'\n",
    "from textblob import TextBlob\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Parts/NNS\n",
      "  of/IN\n",
      "  speech/NN\n",
      "  examples/NNS\n",
      "  :/:\n",
      "  an/DT\n",
      "  article/NN\n",
      "  ,/,\n",
      "  to/TO\n",
      "  write/VB\n",
      "  ,/,\n",
      "  interesting/VBG\n",
      "  ,/,\n",
      "  easily/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  ,/,\n",
      "  of/IN)\n"
     ]
    }
   ],
   "source": [
    "#NLTKを使ってPOCを行う\n",
    "from nltk import word_tokenize,pos_tag,ne_chunk\n",
    "input_str = 'Parts of speech examples: an article, to write, interesting, easily, and, of'\n",
    "print(ne_chunk(pos_tag(word_tokenize(input_str))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ⑧词语分块（浅解析）\n",
    "    词语分块是一种识别句子中的组成部分（如名词、动词、形容词等），并将它们链接到具有不连续语法意义的高阶单元（如名词组或短语、动词组等） 的自然语言过程。常用的词语分块工具包括：NLTK，TreeTagger chunker，Apache OpenNLP，文本工程通用架构（GATE），FreeLing。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('black', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('a', 'DT'), ('white', 'JJ'), ('stove', 'NN'), ('were', 'VBD'), ('bought', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('apartment', 'NN'), ('of', 'IN'), ('John', 'NNP')]\n",
      "(S\n",
      "  (NP A/DT black/JJ television/NN)\n",
      "  and/CC\n",
      "  (NP a/DT white/JJ stove/NN)\n",
      "  were/VBD\n",
      "  bought/VBN\n",
      "  for/IN\n",
      "  (NP the/DT new/JJ apartment/NN)\n",
      "  of/IN\n",
      "  John/NNP)\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "no display name and no $DISPLAY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8155e9ad3f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *trees)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NLTK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<Control-x>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
     ]
    }
   ],
   "source": [
    "#1,単語ごとのpart of speeckを確定する\n",
    "input_str = 'A black television and a white stove were bought for the new apartment of John.'\n",
    "from textblob import TextBlob\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)\n",
    "\n",
    "#2,単語の分割を行う\n",
    "import IPython\n",
    "from IPython.display import Image, display\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# tree = Tree.fromstring('(S (NP this tree) (VP (V is) (AdjP pretty)))')\n",
    "# display(tree)\n",
    "\n",
    "reg_exp = 'NP:{<DT>?<JJ>*<NN>}'\n",
    "rp = nltk.RegexpParser(reg_exp)\n",
    "result = rp.parse(result.tags)\n",
    "print(result)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ⑨Named Entity Recognition（NER）\n",
    "    命名实体识别（NER） 旨在从文本中找到命名实体，并将它们划分到事先预定义的类别（人员、地点、组织、时间等）\n",
    "    \n",
    "    常见的命名实体识别工具如下表所示，包括：NLTK，spaCy，文本工程通用架构（GATE） -- ANNIE，Apache OpenNLP，Stanford CoreNLP，DKPro核心，MITIE，Watson NLP，TextRazor，FreeLing 等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ➉共指解析 Coreference resolution（回指分辨率 anaphora resolution）\n",
    "    代词和其他引用表达应该与正确的个体联系起来。Coreference resolution 在文本中指的是引用真实世界中的同一个实体。如在句子 “安德鲁说他会买车”中，代词“他”指的是同一个人，即“安德鲁”。常用的 Coreference resolution 工具如下表所示，包括 Stanford CoreNLP，spaCy，Open Calais，Apache OpenNLP 等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ⑪搭配提取（Collocation extraction）\n",
    "    ①搭配提取过程并不是单独、偶然发生的，它是与单词组合一同发生的过程。该过程的示例包括“打破规则 break the rules”，“空闲时间 free time”，“得出结论 draw a conclusion”，“记住 keep in mind”，“准备好 get ready”等\n",
    "    ②使用 ICE 实现搭配提取\n",
    "    ICE参考資料：\n",
    "    https://doc.zeroc.com/ice/3.7/release-notes/using-the-python-distribution#id-.UsingthePythonDistributionv3.7-InstallingtheIceforPythonDistribution\n",
    "    →cannot import name 'CollocationExtractor' from 'Ice'エラー未解決\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CollocationExtractor' from 'Ice' (C:\\Users\\yu_wang\\Anaconda3\\envs\\Python38_new\\lib\\site-packages\\Ice\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ac2a6c34f67d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minput_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'he and Chazz duel with all keys on the line.'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mIce\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCollocationExtractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m extractor = ice.CollocationExtractor.with_collocation_pipeline('T1' , bing_key = 'Temp',\n\u001b[0;32m      4\u001b[0m                                                                pos_check = False)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CollocationExtractor' from 'Ice' (C:\\Users\\yu_wang\\Anaconda3\\envs\\Python38_new\\lib\\site-packages\\Ice\\__init__.py)"
     ]
    }
   ],
   "source": [
    "input_str = ['he and Chazz duel with all keys on the line.']\n",
    "from Ice import CollocationExtractor\n",
    "extractor = ice.CollocationExtractor.with_collocation_pipeline('T1' , bing_key = 'Temp',\n",
    "                                                               pos_check = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ⑫关系提取（Relationship extraction）\n",
    "    关系提取过程是指从非结构化的数据源 （如原始文本）获取结构化的文本信息。严格来说，它确定了命名实体（如人、组织、地点的实体） 之间的关系（如配偶、就业等关系）。例如，从“昨天与 Mark 和 Emily 结婚”这句话中，我们可以提取到的信息是 Mark 是 Emily 的丈夫。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
