{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/fronteo/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: click in /home/fronteo/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /home/fronteo/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages (from nltk>=3.1->textblob) (4.48.0)\n",
      "Requirement already satisfied: joblib in /home/fronteo/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages (from nltk>=3.1->textblob) (0.17.0)\n",
      "Requirement already satisfied: regex in /home/fronteo/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages (from nltk>=3.1->textblob) (2020.10.28)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/fronteo/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ①よくある文章のノーマライゼーション処理\n",
    "    https://mp.weixin.qq.com/s/Gq_0Ks3XTUzZSFYjRrcfMQ\n",
    "    ・大文字・小文字の統一\n",
    "    ・数字を単語にするか、削除\n",
    "    ・文章区切り等記号の処理\n",
    "    ・空白の削除\n",
    "    ・省略語の修正\n",
    "    ・特殊表現の処理（削除等）\n",
    "    ・text canonicalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ①大文字・小文字の統一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n",
      "THE 5 BIGGEST COUNTRIES BY POPULATION IN 2017 ARE CHINA, INDIA, UNITED STATES, INDONESIA, AND BRAZIL.\n"
     ]
    }
   ],
   "source": [
    "input_str = 'The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.'\n",
    "input_str = input_str.lower()\n",
    "print(input_str)\n",
    "input_str = input_str.upper()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ②数字を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "input_str = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n",
    "result = re.sub(r'\\d+','',input_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ★★★★➂文章区切り等記号の処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "input_str = 'This &is [an] example? {of} string. with.? punctuation!!!!'\n",
    "trans=str.maketrans({key: None for key in string.punctuation})\n",
    "result=input_str.translate(trans)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ④空白を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a string example\n"
     ]
    }
   ],
   "source": [
    "input_str = ' \\t a string example\\t '\n",
    "input_str = input_str.strip()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ④stopwords\n",
    "    nltk,scikit-learn,spaCy等stopwordsを提供してくれる\n",
    "    from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "#英文(nltk)  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "input_str = 'NLTK is a leading platform for building Python programs to work with human language data.'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ➄単語語根の抽出(Stemming)\n",
    "単語の語根（books→book　looked→look）\n",
    "2種類のやり方：\n",
    "Porter stemming（単語内のよくある形態または語尾を削除）\n",
    "Lancaster stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/fronteo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#NLTKを使って語根を抽出する\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "input_str = 'There are several types of stemming algorithms.'\n",
    "input_str = word_tokenize(input_str) #分かち書き\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ➅単語のステムリダクション（詞幹還原）\n",
    "    単語のステムリダクションとは、単語ごとの形態を統一するためである。語根抽出と異なり、単語のステムリダクションはモジュールを使って単語の形態を変更する\n",
    "    \n",
    "    当前常用的词形还原工具库包括： NLTK（WordNet Lemmatizer），spaCy，TextBlob，Pattern，gensim，Stanford CoreNLP，基于内存的浅层解析器（MBSP），Apache OpenNLP，Apache Lucene，文本工程通用架构（GATE），Illinois Lemmatizer 和 DKPro Core。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posあり:been\n",
      "posなし:been\n",
      "======================================\n",
      "posあり:had\n",
      "posなし:had\n",
      "======================================\n",
      "posあり:done\n",
      "posなし:done\n",
      "======================================\n",
      "posあり:languages\n",
      "posなし:language\n",
      "======================================\n",
      "posあり:cities\n",
      "posなし:city\n",
      "======================================\n",
      "posあり:mice\n",
      "posなし:mouse\n",
      "======================================\n",
      "posあり:good\n",
      "posなし:better\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "input_str = 'been had done languages cities mice better'\n",
    "input_str = word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print('posあり:'+str(lemmatizer.lemmatize(word,pos = 'a')))\n",
    "     \n",
    "    print('posなし:'+str(lemmatizer.lemmatize(word)))\n",
    "    \n",
    "    print('======================================')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ⑦Part-of-speech tagging（POC）\n",
    "    当前有许多包含 POS 标记器的工具，包括 NLTK，spaCy，TextBlob，Pattern，Stanford CoreNLP，基于内存的浅层分析器（MBSP），Apache OpenNLP，Apache Lucene，文本工程通用架构（GATE），FreeLing，Illinois Part of Speech Tagger 和 DKPro Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "input_str = 'Parts of speech examples: an article, to write, interesting, easily, and, of'\n",
    "from textblob import TextBlob\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ⑧词语分块（浅解析）\n",
    "    词语分块是一种识别句子中的组成部分（如名词、动词、形容词等），并将它们链接到具有不连续语法意义的高阶单元（如名词组或短语、动词组等） 的自然语言过程。常用的词语分块工具包括：NLTK，TreeTagger chunker，Apache OpenNLP，文本工程通用架构（GATE），FreeLing。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
